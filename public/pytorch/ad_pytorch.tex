\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{color}
\usepackage{listings}

\lstset{basicstyle=\ttfamily,breaklines=true}
\usepackage{fontspec}
\makeatletter
\def\verbatim@nolig@list{}
\makeatother
\setmonofont{JetBrains Mono}[Contextuals=Alternate]

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{adjustbox}
\usepackage{unicode-math}
\usepackage[pdf]{graphviz}

\title{AD in PyTorch}
\titlegraphic{\includegraphics[width=0.3\textwidth]{pytorch}}
\author{Breandan Considine}

\begin{document}
    \frame{\titlepage}


    \begin{frame}
        \frametitle{What is automatic differentiation?}
        To understand AD, you just need to remember two simple rules:
        \begin{align*}
            D(f + g) &= D(f) + D(g) \\
            D(f \cdot g) &= D(f) \cdot g + f \cdot D(g)
        \end{align*}
        We can think of AD as a \textit{linear map} between functions.

        \begin{align*}
            D(f + g) &= D(f) + D(g) \\
            D(f \cdot g) &= D(f) \cdot g + f \cdot D(g)
        \end{align*}
    \end{frame}


    \begin{frame}
        \frametitle{Higher order and higher rank AD}

        The \textit{gradient}, $\nabla: (\mathbb{R}^m\rightarrow\mathbb{R})\rightarrow\mathbb{R}^m$ maps a function $Q$ to:

        \begin{equation*}
            \nabla Q(q_1, \dots, q_m) = \left[ \frac{\partial Q}{\partial q_1}, \dots, \frac{\partial Q}{\partial q_m}\right]
        \end{equation*}

        The \textit{Jacobian}, $\mathcal{J}: (\mathbb{R}^m\rightarrow\mathbb{R}^n)\rightarrow\mathbb{R}^{n \times m}$ is a matrix of partials:

        \begin{equation*}
            \mathcal{J}\circ\mathbf{f} =
            \begin{bmatrix}
                \dfrac{\partial \mathbf{f}}{\partial x_1} & \cdots & \dfrac{\partial \mathbf{f}}{\partial x_m}
            \end{bmatrix} =
            \begin{bmatrix}
                \dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_m}\\
                \vdots & \ddots & \vdots\\
                \dfrac{\partial f_n}{\partial x_1} & \cdots & \dfrac{\partial f_n}{\partial x_m}
            \end{bmatrix} =
            \begin{bmatrix}
                \nabla f_1 \\
                \vdots \\
                \nabla f_m
            \end{bmatrix}
        \end{equation*}
    \end{frame}

    \begin{frame}[fragile]
        \frametitle{Jacobians in PyTorch}
        \begin{lstlisting}[language=Python]
def jacobian(fun, x) -> torch.Tensor:
    x = x.detach().requires_grad_()
    y = fun(x)
    def vjp(v) -> torch.Tensor:
        return torch.autograd.grad(y, x, v)[0]

    vs = torch.eye(y.numel())\
              .view(y.numel(), *y.shape)
    result = vmap(vjp)(vs)
    return result.detach()

f = lambda x: x ** 3
jacobian(f, torch.ones(3))
        \end{lstlisting}
    \end{frame}

    \begin{frame}
        \frametitle{Higher order and higher rank AD}

        Suppose we have a function $P(X) = p_k \circ p_{k-1} \circ \cdots \circ p_1 \circ X$. The derivative of a linear composition can be expressed as a product:

        \begin{equation*} \label{eq:sfun_chain_rule}
        \frac{dP}{dp_1} = \frac{dp_k}{dp_{k-1}}\frac{dp_{k-1}}{dp_{k-2}}\dots\frac{dp_2}{dp_1}= {\displaystyle \prod_{i=1}^{k-1} \frac{dp_{i+1}}{dp_{i}}}
        \end{equation*}

        This also holds in higher dimensions, for example $\mathbf{P}_k: \mathbb{R}^m\rightarrow\mathbb{R}^n$:

        \begin{align*} \label{eq:vfun_chain_rule}
        \mathcal{J} \mathbf{P_k} = \displaystyle \prod_{i=1}^{k} \mathcal{J}p_i &= \underbrace{\bigg(\Big((\mathcal{J}p_k \mathcal{J}p_{k-1}) \dots \mathcal{J}p_2\Big) \mathcal{J}p_1\bigg)}_{\textit{Reverse mode, VJP, Pullback}} \\
        &= \underbrace{\bigg(\mathcal{J}p_k \Big(\mathcal{J}p_{k-1} \dots (\mathcal{J}p_2 \mathcal{J}p_1)\Big)\bigg)}_{\textit{Forward mode, JVP, Pushforward}}
        \end{align*}
    \end{frame}

    \begin{frame}
        \frametitle{Higher order and higher rank AD}

        The \textit{Hessian} $\mathbf{H}:(\mathbb{R}^m\rightarrow\mathbb{R})\rightarrow\mathbb{R}^{m\times m}$ maps scalar functions to $\partial^2$:

        \begin{equation*}
            \mathbf{H}(Q) = \begin{bmatrix}{\dfrac {\partial ^{2}Q}{\partial x_{1}^{2}}}&{\dfrac {\partial ^{2}Q}{\partial x_{1}\,\partial x_{2}}}&\cdots &{\dfrac {\partial ^{2}Q}{\partial x_{1}\,\partial x_{m}}}\\[2.2ex]{\dfrac {\partial ^{2}Q}{\partial x_{2}\,\partial x_{1}}}&{\dfrac {\partial ^{2}Q}{\partial x_{2}^{2}}}&\cdots &{\dfrac {\partial ^{2}Q}{\partial x_{2}\,\partial x_{m}}}\\[2.2ex]\vdots &\vdots &\ddots &\vdots \\[2.2ex]{\dfrac {\partial ^{2}Q}{\partial x_{m}\,\partial x_{1}}}&{\dfrac {\partial ^{2}Q}{\partial x_{m}\,\partial x_{2}}}&\cdots &{\dfrac {\partial ^{2}Q}{\partial x_{m}^{2}}}\end{bmatrix}
        \end{equation*}

        The Hessian and Jacobian are related by $\mathbf{H}(Q)^\intercal = \mathcal{J} \circ \nabla Q$.
    \end{frame}

    \begin{frame}[fragile]
        \frametitle{Hessians in PyTorch}
        \begin{lstlisting}[language=Python]
def hessian(fun, x) -> torch.Tensor:
    def grad0(x: torch.Tensor):
        y = fun(x)
        assert y.dim() == 0
        return torch.autograd.grad(y, x, create_graph=True)[0]
    return jacobian(grad0, x)

g = lambda x: (x ** 3).sum()
hessian(g, torch.ones(3))
        \end{lstlisting}
    \end{frame}

    \begin{frame}[fragile]
        \frametitle{Static vs. Dynamic ADs: Representations}
        \begin{table}[H]
            \centering
            \begin{tabular}{lll}
                \textbf{Program} & & \textbf{Computation Graph} \\\\
%            \begin{tabular}[c]{@{}l@{}} $\hat y = θx + b$\\ $l = ||\hat y - y||_2$\end{tabular} &
                    \begin{lstlisting}[basicstyle=\ttfamily\footnotesize, language=Python]
sum = 0
l = [0, 0, 0, 0]
for i in range(0, 4):
  l[i] += t[i] * x[i]
for i in range(0, 4):
  l[i] -= y[i] - b
for i in range(0, 4):
  l[i] *= l[i]
for i in range(0, 4):
  sum += l[i]
l = sqrt(sum)
                    \end{lstlisting} & &
                \begin{adjustbox}{minipage={.4\textwidth}, height=0.22\textwidth, margin*=0cm 0cm 0cm 0.1cm}
                      \digraph[scale=0.1]{prograph}{
                          node[ fontname="Helvetica" fontsize=20 shape=Mrecord ];
                          edge[ fontname="Helvetica" fontsize=18 ];

                          graph ["concentrate"="true","rankdir"="LR","bgcolor"="transparent","margin"="0.0","compound"="true","nslimit"="20"]
                          "eeba8" ["label"="+"]
                          "a8416" ["label"="+"]
                          "4500f" ["label"="pow"]
                          "a67f9" ["label"="*"]
                          "0.5" ["label"="0.5"]
                          "f14a3" ["label"="*"]
                          "9c49d" ["label"="*"]
                          "59c48" ["label"="+"]
                          "980bd" ["label"="+"]
                          "8f532" ["label"="+"]
                          "1a609" ["label"="+"]
                          "e58c4" ["label"="+"]
                          "23f5b" ["label"="+"]
                          "d829b" ["label"="+"]
                          "y₀" ["label"="y₀"]
                          "2783d" ["label"="+"]
                          "8bd47" ["label"="+"]
                          "y₂" ["label"="y₂"]
                          "517e6" ["label"="+"]
                          "8caa0" ["label"="+"]
                          "y₁" ["label"="y₁"]
                          "7da0d" ["label"="+"]
                          "b12cb" ["label"="+"]
                          "b" ["label"="b"]
                          "f8941" ["label"="+"]
                          "3eecd" ["label"="+"]
                          "2e83a" ["label"="+"]
                          "b59fd" ["label"="+"]
                          "dae83" ["label"="+"]
                          "b11ba" ["label"="*"]
                          "3bb89" ["label"="*"]
                          "b2454" ["label"="*"]
                          "7bed4" ["label"="*"]
                          "39644" ["label"="*"]
                          "12c32" ["label"="*"]
                          "d58d1" ["label"="*"]
                          "6c64d" ["label"="*"]
                          "fb0f0" ["label"="*"]
                          "6c2be" ["label"="*"]
                          "57fd4" ["label"="*"]
                          "a9bc3" ["label"="*"]
                          "x₀" ["label"="x₀"]
                          "θ₀" ["label"="θ₀"]
                          "x₂" ["label"="x₂"]
                          "θ₁" ["label"="θ₁"]
                          "x₂" ["label"="x₂"]
                          "x₄" ["label"="x₄"]
                          "x₁" ["label"="x₁"]
                          "x₃" ["label"="x₃"]
                          "eeba8" -> "a8416"
                          "a8416" -> "4500f"
                          "a67f9" -> "a8416"
                          "0.5" -> "4500f"
                          "f14a3" -> "eeba8"
                          "9c49d" -> "eeba8"
                          "59c48" -> "a67f9"
                          "980bd" -> "a67f9"
                          "8f532" -> "f14a3"
                          "1a609" -> "f14a3"
                          "e58c4" -> "9c49d"
                          "23f5b" -> "9c49d"
                          "d829b" -> "59c48"
                          "y₀" -> "59c48"
                          "y₀" -> "980bd"
                          "2783d" -> "980bd"
                          "8bd47" -> "8f532"
                          "y₂" -> "8f532"
                          "y₂" -> "1a609"
                          "517e6" -> "1a609"
                          "8caa0" -> "e58c4"
                          "y₁" -> "e58c4"
                          "y₁" -> "23f5b"
                          "7da0d" -> "23f5b"
                          "b12cb" -> "d829b"
                          "b" -> "d829b"
                          "b" -> "2783d"
                          "b" -> "8bd47"
                          "b" -> "517e6"
                          "b" -> "8caa0"
                          "b" -> "7da0d"
                          "f8941" -> "2783d"
                          "3eecd" -> "8bd47"
                          "2e83a" -> "517e6"
                          "b59fd" -> "8caa0"
                          "dae83" -> "7da0d"
                          "b11ba" -> "b12cb"
                          "3bb89" -> "b12cb"
                          "b2454" -> "f8941"
                          "7bed4" -> "f8941"
                          "39644" -> "3eecd"
                          "12c32" -> "3eecd"
                          "d58d1" -> "2e83a"
                          "6c64d" -> "2e83a"
                          "fb0f0" -> "b59fd"
                          "6c2be" -> "b59fd"
                          "57fd4" -> "dae83"
                          "a9bc3" -> "dae83"
                          "x₀" -> "b11ba"
                          "x₀" -> "b2454"
                          "θ₀" -> "b11ba"
                          "θ₀" -> "b2454"
                          "θ₀" -> "39644"
                          "θ₀" -> "d58d1"
                          "θ₀" -> "fb0f0"
                          "θ₀" -> "57fd4"
                          "x₂" -> "3bb89"
                          "x₂" -> "7bed4"
                          "θ₁" -> "3bb89"
                          "θ₁" -> "7bed4"
                          "θ₁" -> "12c32"
                          "θ₁" -> "6c64d"
                          "θ₁" -> "6c2be"
                          "θ₁" -> "a9bc3"
                          "x₂" -> "39644"
                          "x₂" -> "d58d1"
                          "x₄" -> "12c32"
                          "x₄" -> "6c64d"
                          "x₁" -> "fb0f0"
                          "x₁" -> "57fd4"
                          "x₃" -> "6c2be"
                          "x₃" -> "a9bc3"
                      } \end{adjustbox}
            \end{tabular}
        \end{table}
    \end{frame}

%    \begin{frame}
%      \frametitle{AD in PyTorch}
%      \begin{enumerate}
%        \item Introduction to AD
%        \item Implicit vs.
%      \end{enumerate}
%    \end{frame}
%
%    \begin{frame}
%      \frametitle{Stateful vs. stateless computation}
%      \begin{enumerate}
%        \item Introduction to AD
%        \item Implicit vs.
%      \end{enumerate}
%    \end{frame}


\end{document}